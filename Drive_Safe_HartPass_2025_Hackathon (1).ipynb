{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndRQkMOAnMQJ",
        "outputId": "ce74d83f-7afa-4db8-cf9b-091bba5c3ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Drive unmounted\n",
            "Mounted at /content/drive\n",
            "Drive mounted\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Unmount the drive first\n",
        "drive.flush_and_unmount()\n",
        "print('Drive unmounted')\n",
        "\n",
        "# Remove existing files from the mountpoint if it exists\n",
        "import os\n",
        "if os.path.exists('/content/drive'):\n",
        "  !rm -rf '/content/drive'  # Use with caution! This permanently deletes all files in the directory.\n",
        "  print('Files removed from mountpoint')\n",
        "\n",
        "# Remount the drive\n",
        "drive.mount('/content/drive')\n",
        "print('Drive mounted')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/Shareddrives/DrunkKiddo'"
      ],
      "metadata": {
        "id": "eKx1c7M_n23Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf # importing tensorflow\n",
        "from tensorflow.keras.applications import MobileNetV2 #this is the model we will be building off of\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D # I have no clue what this does, but thank his excellency, ChatGPT for this\n",
        "from tensorflow.keras.models import Model #Once again, Lord GPT's suggestion"
      ],
      "metadata": {
        "id": "GVA8YXI645tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) # this imports out model\n",
        "#let us explain a little bit about this:\n",
        "# MoveNetV2 is our big, beutiful, light-weight model\n",
        "#we set include_top = False because we want to remove that very last layer of our network, because we do not care about all of the various classifications\n",
        "#that already exist for our data set\n",
        "#finally the input_shape(224, 224, 3) refers to the input image size, as well as the number of pixel types used (3, since RGB)"
      ],
      "metadata": {
        "id": "6j-T1xj-6H8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source directory and the target directories\n",
        "source_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images\"\n",
        "sober_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/sober\"\n",
        "drunk_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/drunk\"\n",
        "\n",
        "# Create target directories if they don't exist\n",
        "os.makedirs(sober_dir, exist_ok=True)\n",
        "os.makedirs(drunk_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through the images in the source directory\n",
        "for filename in os.listdir(source_dir):\n",
        "    source_file = os.path.join(source_dir, filename)\n",
        "    if filename.startswith(\"Soberface\"):\n",
        "        target_file = os.path.join(sober_dir, filename)\n",
        "    elif filename.startswith(\"Drunkface\"):\n",
        "        target_file = os.path.join(drunk_dir, filename)\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        shutil.move(source_file, target_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {source_file}\")\n",
        "\n",
        "print(\"Images have been separated into 'sober' and 'drunk' directories.\")"
      ],
      "metadata": {
        "id": "yf2F5z_ontrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26729c88-edee-48df-bd48-8cadbdde6b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been separated into 'sober' and 'drunk' directories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the source directories for sober and drunk images\n",
        "sober_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/sober\"\n",
        "drunk_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/drunk\"\n",
        "\n",
        "# Create lists of all images\n",
        "sober_images = [os.path.join(sober_dir, img) for img in os.listdir(sober_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "drunk_images = [os.path.join(drunk_dir, img) for img in os.listdir(drunk_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Combine the lists and create labels\n",
        "all_images = sober_images + drunk_images\n",
        "labels = [0] * len(sober_images) + [1] * len(drunk_images)  # 0 for sober, 1 for drunk\n",
        "\n",
        "# Shuffle the data\n",
        "combined = list(zip(all_images, labels))\n",
        "random.shuffle(combined)\n",
        "all_images, labels = zip(*combined)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(all_images, labels, test_size=0.2, random_state=42)\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "# Define the target directories for train, validation, and test sets\n",
        "train_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/train\"\n",
        "val_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/val\"\n",
        "test_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/test\"\n",
        "\n",
        "# Create target directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Function to copy images to their respective directories\n",
        "def copy_images(images, labels, target_dir):\n",
        "    for img, label in zip(images, labels):\n",
        "        label_dir = \"sober\" if label == 0 else \"drunk\"\n",
        "        os.makedirs(os.path.join(target_dir, label_dir), exist_ok=True)\n",
        "        shutil.copy(img, os.path.join(target_dir, label_dir, os.path.basename(img)))\n",
        "\n",
        "# Copy images to train, validation, and test directories\n",
        "copy_images(train_images, train_labels, train_dir)\n",
        "copy_images(val_images, val_labels, val_dir)\n",
        "copy_images(test_images, test_labels, test_dir)\n",
        "\n",
        "print(\"Data has been shuffled and split into train, validation, and test sets.\")"
      ],
      "metadata": {
        "id": "n3Dp_IR7nvz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c0acd2-ccf5-4759-dc94-7c42b2c5d2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been shuffled and split into train, validation, and test sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Directories for training, validation, and testing datasets\n",
        "train_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/train\"\n",
        "val_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/val\"\n",
        "test_dir = \"/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/test\"\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Only rescaling for validation and testing\n",
        "val_test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Generating training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Generating validation data\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Generating test data\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Building the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "70S5RZUHn1ST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f2fe0f-2c17-4088-d926-39e1070b66be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2992 images belonging to 2 classes.\n",
            "Found 998 images belonging to 2 classes.\n",
            "Found 998 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 2s/step - accuracy: 0.5847 - loss: 0.7848 - val_accuracy: 0.7465 - val_loss: 0.5116\n",
            "Epoch 2/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 2s/step - accuracy: 0.7280 - loss: 0.5419 - val_accuracy: 0.7285 - val_loss: 0.5576\n",
            "Epoch 3/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step - accuracy: 0.7721 - loss: 0.4898 - val_accuracy: 0.7906 - val_loss: 0.4525\n",
            "Epoch 4/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 2s/step - accuracy: 0.8139 - loss: 0.4191 - val_accuracy: 0.7946 - val_loss: 0.4394\n",
            "Epoch 5/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 2s/step - accuracy: 0.8144 - loss: 0.4159 - val_accuracy: 0.8116 - val_loss: 0.4357\n",
            "Epoch 6/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 2s/step - accuracy: 0.8201 - loss: 0.4174 - val_accuracy: 0.8246 - val_loss: 0.4077\n",
            "Epoch 7/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 2s/step - accuracy: 0.8392 - loss: 0.3635 - val_accuracy: 0.8327 - val_loss: 0.3751\n",
            "Epoch 8/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 2s/step - accuracy: 0.8598 - loss: 0.3305 - val_accuracy: 0.8277 - val_loss: 0.4077\n",
            "Epoch 9/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 2s/step - accuracy: 0.8478 - loss: 0.3513 - val_accuracy: 0.8667 - val_loss: 0.3306\n",
            "Epoch 10/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 2s/step - accuracy: 0.8743 - loss: 0.2926 - val_accuracy: 0.8487 - val_loss: 0.3699\n",
            "Epoch 11/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 2s/step - accuracy: 0.8714 - loss: 0.2918 - val_accuracy: 0.8186 - val_loss: 0.3968\n",
            "Epoch 12/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 2s/step - accuracy: 0.8789 - loss: 0.2833 - val_accuracy: 0.8818 - val_loss: 0.2980\n",
            "Epoch 13/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 2s/step - accuracy: 0.8929 - loss: 0.2439 - val_accuracy: 0.8737 - val_loss: 0.3402\n",
            "Epoch 14/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 2s/step - accuracy: 0.9004 - loss: 0.2400 - val_accuracy: 0.8928 - val_loss: 0.2931\n",
            "Epoch 15/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 2s/step - accuracy: 0.9243 - loss: 0.1981 - val_accuracy: 0.8928 - val_loss: 0.2952\n",
            "Epoch 16/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 2s/step - accuracy: 0.9134 - loss: 0.2171 - val_accuracy: 0.9088 - val_loss: 0.2554\n",
            "Epoch 17/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 2s/step - accuracy: 0.9463 - loss: 0.1549 - val_accuracy: 0.9228 - val_loss: 0.2291\n",
            "Epoch 18/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 2s/step - accuracy: 0.9389 - loss: 0.1544 - val_accuracy: 0.9168 - val_loss: 0.2532\n",
            "Epoch 19/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 2s/step - accuracy: 0.9566 - loss: 0.1196 - val_accuracy: 0.9269 - val_loss: 0.2504\n",
            "Epoch 20/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 2s/step - accuracy: 0.9568 - loss: 0.1145 - val_accuracy: 0.9158 - val_loss: 0.2477\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 587ms/step - accuracy: 0.9201 - loss: 0.2164\n",
            "Test accuracy: 0.9088176488876343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/saved_model/', exist_ok=True)\n",
        "\n",
        "# Save the entire model to a HDF5 file\n",
        "model.save('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/model.h5')\n",
        "\n",
        "# For saving the model after training in the recommended Keras format\n",
        "model.save('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/saved_model/my_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkoAdRr5E7k2",
        "outputId": "4494fbae-84cc-4247-9938-ef77896c842d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v__X1A0M-ja",
        "outputId": "97b6d5ef-b007-4cb5-94bf-4137d18edab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load MobileNetV2 and freeze its layers\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)  # Ensure 'output' is a KerasTensor\n",
        "\n",
        "# Integrate with your existing model\n",
        "stronger_model = Model(inputs=base_model.input, outputs=output)\n",
        "stronger_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jMp7ODExNAwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Only rescaling for validation and testing\n",
        "val_test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Generating training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Generating validation data\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    '/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/images/val',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Train the stronger model\n",
        "stronger_model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Save the stronger model in Keras format\n",
        "stronger_model.save('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/saved_model/stronger_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW0jvXb-NDZK",
        "outputId": "753ef01e-172b-4ff5-ae90-1cfcb932d6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2992 images belonging to 2 classes.\n",
            "Found 998 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 2s/step - accuracy: 0.8345 - loss: 0.3645 - val_accuracy: 0.9138 - val_loss: 0.2063\n",
            "Epoch 2/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 2s/step - accuracy: 0.9306 - loss: 0.1907 - val_accuracy: 0.9289 - val_loss: 0.1803\n",
            "Epoch 3/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 2s/step - accuracy: 0.9397 - loss: 0.1552 - val_accuracy: 0.9168 - val_loss: 0.1982\n",
            "Epoch 4/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 2s/step - accuracy: 0.9599 - loss: 0.1078 - val_accuracy: 0.9409 - val_loss: 0.1498\n",
            "Epoch 5/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 2s/step - accuracy: 0.9658 - loss: 0.0917 - val_accuracy: 0.9349 - val_loss: 0.1561\n",
            "Epoch 6/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 3s/step - accuracy: 0.9692 - loss: 0.0922 - val_accuracy: 0.9489 - val_loss: 0.1290\n",
            "Epoch 7/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 2s/step - accuracy: 0.9640 - loss: 0.0867 - val_accuracy: 0.9559 - val_loss: 0.1031\n",
            "Epoch 8/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 2s/step - accuracy: 0.9795 - loss: 0.0539 - val_accuracy: 0.9569 - val_loss: 0.1084\n",
            "Epoch 9/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 2s/step - accuracy: 0.9785 - loss: 0.0655 - val_accuracy: 0.9449 - val_loss: 0.1281\n",
            "Epoch 10/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step - accuracy: 0.9844 - loss: 0.0470 - val_accuracy: 0.9559 - val_loss: 0.1071\n",
            "Epoch 11/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 2s/step - accuracy: 0.9835 - loss: 0.0458 - val_accuracy: 0.9679 - val_loss: 0.0903\n",
            "Epoch 12/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 3s/step - accuracy: 0.9928 - loss: 0.0272 - val_accuracy: 0.9669 - val_loss: 0.0967\n",
            "Epoch 13/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 2s/step - accuracy: 0.9862 - loss: 0.0369 - val_accuracy: 0.9649 - val_loss: 0.0881\n",
            "Epoch 14/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 2s/step - accuracy: 0.9931 - loss: 0.0241 - val_accuracy: 0.9669 - val_loss: 0.0813\n",
            "Epoch 15/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 2s/step - accuracy: 0.9938 - loss: 0.0274 - val_accuracy: 0.9649 - val_loss: 0.0916\n",
            "Epoch 16/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 3s/step - accuracy: 0.9929 - loss: 0.0237 - val_accuracy: 0.9649 - val_loss: 0.0907\n",
            "Epoch 17/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 2s/step - accuracy: 0.9929 - loss: 0.0211 - val_accuracy: 0.9489 - val_loss: 0.1383\n",
            "Epoch 18/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 3s/step - accuracy: 0.9874 - loss: 0.0314 - val_accuracy: 0.9639 - val_loss: 0.0845\n",
            "Epoch 19/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 2s/step - accuracy: 0.9938 - loss: 0.0170 - val_accuracy: 0.9709 - val_loss: 0.0732\n",
            "Epoch 20/20\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 2s/step - accuracy: 0.9919 - loss: 0.0289 - val_accuracy: 0.9689 - val_loss: 0.0688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import json\n",
        "\n",
        "# Load the Keras model\n",
        "model = load_model('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/saved_model/stronger_model.keras')\n",
        "\n",
        "# Convert the model to JSON format\n",
        "model_json = model.to_json()\n",
        "\n",
        "# Save the JSON to a file\n",
        "with open('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/saved_model/stronger_model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Save the weights to a bin file\n",
        "weights = model.get_weights()\n",
        "with open('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/saved_model/stronger_model_weights.bin', 'wb') as bin_file:\n",
        "    for layer_weights in weights:\n",
        "        bin_file.write(layer_weights.tobytes())"
      ],
      "metadata": {
        "id": "wjZx1ZDpdhrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2tdCy3lohz9eStVvQLnJT4BFCQu_2JWFyZoPMeSUqzTaaBUtQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwWiPJeSMpdR",
        "outputId": "22cc5fc3-02c6-4b18-c582-1898db45d013"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Load the Keras model\n",
        "model = load_model('/content/drive/Shareddrives/DrunkKiddo/UJInebriate-main/saved_model/stronger_model.keras')\n",
        "\n",
        "def prepare_image(image, target_size):\n",
        "    image = image.resize(target_size)  # Resize the image\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "def predict_image(image):\n",
        "    image = prepare_image(image, target_size=(224, 224))\n",
        "    prediction = model.predict(image)[0][0]\n",
        "    result = \"Not Drunk\" if prediction > 0.5 else \"Drunk\"\n",
        "    return result\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_image,\n",
        "    inputs=gr.Image(type=\"pil\"),  # Use PIL type to ensure compatibility with .resize()\n",
        "    outputs=\"text\",\n",
        "    title=\"Drunk Detection\",\n",
        "    description=\"Upload an image to predict if the person is drunk or not.\"\n",
        ")\n",
        "\n",
        "# Start Gradio interface and get the public URL using ngrok\n",
        "public_url = ngrok.connect(7860)\n",
        "print(f'Public URL: {public_url}')\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "bVxEg9tWiqgT",
        "outputId": "149c076b-4205-40c9-c458-62a8c79b4652"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://f60f-34-60-85-19.ngrok-free.app\" -> \"http://localhost:7860\"\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://da1177674f7e91bc5a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://da1177674f7e91bc5a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}